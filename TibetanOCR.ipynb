{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsOI-A0IVULx",
        "outputId": "f331cd8b-b0b5-4a92-97cd-9d7cbc7b1771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchvision import transforms\n",
        "\n",
        "from datasets import load_dataset, load_from_disk, DatasetDict, concatenate_datasets\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import json\n",
        "import os\n",
        "from google.colab import drive, files\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjdbmXDHx2Ol",
        "outputId": "b6fcb04b-9e29-4a87-a237-13882c10cfa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "drive_base_path = \"/content/drive/MyDrive/hf_ocr_data\"\n",
        "os.makedirs(drive_base_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVKHq5FzWwub",
        "outputId": "1c4ef7de-96e1-4a27-9dc1-93e35e645693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dataset({\n",
            "    features: ['filename', 'label', 'image_url', 'BDRC_work_id', 'char_len', 'script', 'writing_style', 'print_method'],\n",
            "    num_rows: 223080\n",
            "}), Dataset({\n",
            "    features: ['filename', 'label', 'image_url', 'BDRC_work_id', 'char_len', 'script', 'writing_style', 'print_method'],\n",
            "    num_rows: 41784\n",
            "}), Dataset({\n",
            "    features: ['filename', 'label', 'image_url', 'BDRC_work_id', 'char_len', 'script', 'writing_style', 'print_method'],\n",
            "    num_rows: 16326\n",
            "}), Dataset({\n",
            "    features: ['filename', 'label', 'image_url', 'BDRC_work_id', 'char_len', 'script', 'writing_style', 'print_method'],\n",
            "    num_rows: 1217\n",
            "}), Dataset({\n",
            "    features: ['filename', 'label', 'image_url', 'BDRC_work_id', 'char_len', 'script', 'writing_style', 'print_method'],\n",
            "    num_rows: 75168\n",
            "}), Dataset({\n",
            "    features: ['filename', 'label', 'image_url', 'BDRC_work_id', 'char_len', 'script', 'writing_style', 'print_method'],\n",
            "    num_rows: 1317\n",
            "}), Dataset({\n",
            "    features: ['filename', 'label', 'image_url', 'BDRC_work_id', 'char_len', 'script', 'writing_style', 'print_method'],\n",
            "    num_rows: 595\n",
            "}), Dataset({\n",
            "    features: ['filename', 'label', 'image_url', 'BDRC_work_id', 'char_len', 'script', 'writing_style', 'print_method'],\n",
            "    num_rows: 143\n",
            "}), Dataset({\n",
            "    features: ['filename', 'label', 'image_url', 'BDRC_work_id', 'char_len', 'script', 'writing_style', 'print_method'],\n",
            "    num_rows: 61656\n",
            "}), Dataset({\n",
            "    features: ['filename', 'label', 'image_url', 'BDRC_work_id', 'char_len', 'script', 'writing_style', 'print_method'],\n",
            "    num_rows: 317\n",
            "})]\n"
          ]
        }
      ],
      "source": [
        "dataset_dict = load_dataset(\"MonlamAI/OCR-Tibetan_line_to_text_benchmark\")\n",
        "\n",
        "def is_valid(example):\n",
        "    return example['label'] is not None and len(example['label'].strip()) > 0\n",
        "\n",
        "filtered_splits = [\n",
        "    dataset_dict[split].filter(is_valid)\n",
        "    for split in dataset_dict.keys()\n",
        "]\n",
        "\n",
        "print(filtered_splits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_f32PHLXDeg",
        "outputId": "0a98ef0b-d1fb-41fd-8807-260784879245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['filename', 'label', 'image_url', 'BDRC_work_id', 'char_len', 'script', 'writing_style', 'print_method'],\n",
            "    num_rows: 379442\n",
            "})\n",
            "Dataset({\n",
            "    features: ['filename', 'label', 'image_url', 'BDRC_work_id', 'char_len', 'script', 'writing_style', 'print_method'],\n",
            "    num_rows: 42161\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "full_dataset = concatenate_datasets(filtered_splits)\n",
        "\n",
        "split = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split['train']\n",
        "test_dataset = split['test']\n",
        "\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2GspyKDZMpm",
        "outputId": "907dbd97-f13e-419b-d4bf-65a506c9564f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('\\t', 1), (' ', 2), ('.', 3), (':', 4), ('ऽ', 5), ('।', 6), ('ༀ', 7), ('༄', 8), ('༅', 9), ('༆', 10)]\n",
            "Vocab size (w/ pad): 137\n",
            "Total output classes (with blank): 138\n",
            "CTC blank index: 137\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((64, 512)),\n",
        "    transforms.ToTensor(),          # [C x H x W]\n",
        "])\n",
        "\n",
        "def build_char_vocab(dataset):\n",
        "    counter = Counter()\n",
        "    for text in dataset['label']:\n",
        "        counter.update(text)\n",
        "    chars = sorted(counter)\n",
        "    char2id = {ch: i + 1 for i, ch in enumerate(chars)}  # Reserve 0 for <pad>\n",
        "    char2id['<pad>'] = 0\n",
        "    id2char = {i: ch for ch, i in char2id.items()}\n",
        "    return char2id, id2char\n",
        "\n",
        "char2id, id2char = build_char_vocab(train_dataset)\n",
        "\n",
        "num_classes = len(char2id) + 1         # +1 for CTC blank token\n",
        "ctc_blank_id = len(char2id)            # last index is CTC blank\n",
        "\n",
        "print(list(char2id.items())[:10])\n",
        "print(f\"Vocab size (w/ pad): {len(char2id)}\")\n",
        "print(f\"Total output classes (with blank): {num_classes}\")\n",
        "print(f\"CTC blank index: {ctc_blank_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg1WkjtTwVA4"
      },
      "outputs": [],
      "source": [
        "# Converts images and Tibetan text into tensors + character IDs\n",
        "def process_example(example):\n",
        "    try:\n",
        "        response = requests.get(example['image_url'], timeout=3)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        image = transform(image)  # [1, 64, 512]\n",
        "\n",
        "        label_ids = [char2id.get(c, char2id[\"<pad>\"]) for c in example['label']]\n",
        "\n",
        "        return {\n",
        "            'image_tensor': image,\n",
        "            'label': example['label'],\n",
        "            'label_ids': label_ids\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'image_tensor': torch.zeros(1, 64, 512),  # match expected shape\n",
        "            'label': '',\n",
        "            'label_ids': [char2id[\"<pad>\"]]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORpnSjKtwXMV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "4cc421c8-213b-420c-d931-57238984aae8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# === 4. Determine Already Processed Count ===\\nif os.path.exists(processed_path):\\n    processed_old = load_from_disk(processed_path)\\n    already_processed = len(processed_old)\\n    print(f\"Already processed: {already_processed}\")\\nelse:\\n    processed_old = None\\n    already_processed = 0\\n    print(\"Starting fresh.\")\\n\\n# === 5. Get Next Batch and Preprocess ===\\nbatch_size = 10000\\nend_index = already_processed + batch_size\\n\\nif already_processed >= len(train_dataset):\\n    print(\"All training data has been processed!\")\\nelse:\\n    new_raw_dataset = train_dataset.select(range(already_processed, min(end_index, len(train_dataset))))\\n    print(f\"Processing entries {already_processed} to {min(end_index, len(train_dataset))}\")\\n\\n    processed_new = new_raw_dataset.map(\\n        process_example,\\n        remove_columns=new_raw_dataset.column_names,\\n        num_proc=4  # Adjust based on your CPU cores\\n    )\\n\\n    # === 6. Merge and Save Safely ===\\n    if processed_old:\\n        merged_dataset = concatenate_datasets([processed_old, processed_new])\\n    else:\\n        merged_dataset = processed_new\\n\\n    # SAFE SAVE using temp path\\n    merged_dataset.save_to_disk(tmp_path)\\n\\n    # Replace old folder with new one\\n    if os.path.exists(processed_path):\\n        shutil.rmtree(processed_path)\\n    shutil.move(tmp_path, processed_path)\\n\\n    print(\"Updated processed dataset saved.\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "import shutil\n",
        "drive_base_path = \"/content/drive/MyDrive/hf_ocr_data\"\n",
        "os.makedirs(drive_base_path, exist_ok=True)\n",
        "\n",
        "processed_path = os.path.join(drive_base_path, \"processed_train\")\n",
        "tmp_path = os.path.join(drive_base_path, \"processed_train_tmp\")\n",
        "'''\n",
        "if os.path.exists(processed_path):\n",
        "    processed_old = load_from_disk(processed_path)\n",
        "    already_processed = len(processed_old)\n",
        "    print(f\"Already processed: {already_processed}\")\n",
        "else:\n",
        "    processed_old = None\n",
        "    already_processed = 0\n",
        "    print(\"Starting fresh.\")\n",
        "\n",
        "batch_size = 10000\n",
        "end_index = already_processed + batch_size\n",
        "\n",
        "if already_processed >= len(train_dataset):\n",
        "    print(\"All training data has been processed!\")\n",
        "else:\n",
        "    new_raw_dataset = train_dataset.select(range(already_processed, min(end_index, len(train_dataset))))\n",
        "    print(f\"Processing entries {already_processed} to {min(end_index, len(train_dataset))}\")\n",
        "\n",
        "    processed_new = new_raw_dataset.map(\n",
        "        process_example,\n",
        "        remove_columns=new_raw_dataset.column_names,\n",
        "        num_proc=4  # Adjust based on your CPU cores\n",
        "    )\n",
        "\n",
        "    if processed_old:\n",
        "        merged_dataset = concatenate_datasets([processed_old, processed_new])\n",
        "    else:\n",
        "        merged_dataset = processed_new\n",
        "\n",
        "    merged_dataset.save_to_disk(tmp_path)\n",
        "\n",
        "    if os.path.exists(processed_path):\n",
        "        shutil.rmtree(processed_path)\n",
        "    shutil.move(tmp_path, processed_path)\n",
        "\n",
        "    print(\"Updated processed dataset saved.\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EEa8tbP77XW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "49829189-e17b-46f2-90f4-d6c4762506a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# === Check already processed count for test dataset ===\\nif os.path.exists(test_processed_path):\\n    processed_old_test = load_from_disk(test_processed_path)\\n    already_processed_test = len(processed_old_test)\\n    print(f\"Test set already processed: {already_processed_test}\")\\nelse:\\n    processed_old_test = None\\n    already_processed_test = 0\\n    print(\"Starting fresh test preprocessing.\")\\n\\n# === Process next batch of test data ===\\nbatch_size = 10000\\nend_index_test = already_processed_test + batch_size\\n\\nif already_processed_test >= len(test_dataset):\\n    print(\"All test data has been processed!\")\\nelse:\\n    new_raw_test = test_dataset.select(range(already_processed_test, min(end_index_test, len(test_dataset))))\\n    print(f\"Processing test entries {already_processed_test} to {min(end_index_test, len(test_dataset))}\")\\n\\n    processed_new_test = new_raw_test.map(\\n        process_example,\\n        remove_columns=new_raw_test.column_names,\\n        num_proc=4  # adjust as needed\\n    )\\n\\n    # Merge and save safely\\n    if processed_old_test:\\n        merged_test = concatenate_datasets([processed_old_test, processed_new_test])\\n    else:\\n        merged_test = processed_new_test\\n\\n    merged_test.save_to_disk(test_tmp_path)\\n\\n    if os.path.exists(test_processed_path):\\n        shutil.rmtree(test_processed_path)\\n    shutil.move(test_tmp_path, test_processed_path)\\n\\n    print(\"Updated processed test dataset saved.\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "# Adjust paths as per your Drive setup\n",
        "drive_base_path = \"/content/drive/MyDrive/hf_ocr_data\"\n",
        "test_processed_path = os.path.join(drive_base_path, \"processed_test\")\n",
        "test_tmp_path = os.path.join(drive_base_path, \"processed_test_tmp\")\n",
        "'''\n",
        "if os.path.exists(test_processed_path):\n",
        "    processed_old_test = load_from_disk(test_processed_path)\n",
        "    already_processed_test = len(processed_old_test)\n",
        "    print(f\"Test set already processed: {already_processed_test}\")\n",
        "else:\n",
        "    processed_old_test = None\n",
        "    already_processed_test = 0\n",
        "    print(\"Starting fresh test preprocessing.\")\n",
        "\n",
        "batch_size = 10000\n",
        "end_index_test = already_processed_test + batch_size\n",
        "\n",
        "if already_processed_test >= len(test_dataset):\n",
        "    print(\"All test data has been processed!\")\n",
        "else:\n",
        "    new_raw_test = test_dataset.select(range(already_processed_test, min(end_index_test, len(test_dataset))))\n",
        "    print(f\"Processing test entries {already_processed_test} to {min(end_index_test, len(test_dataset))}\")\n",
        "\n",
        "    processed_new_test = new_raw_test.map(\n",
        "        process_example,\n",
        "        remove_columns=new_raw_test.column_names,\n",
        "        num_proc=4  # adjust as needed\n",
        "    )\n",
        "\n",
        "    if processed_old_test:\n",
        "        merged_test = concatenate_datasets([processed_old_test, processed_new_test])\n",
        "    else:\n",
        "        merged_test = processed_new_test\n",
        "\n",
        "    merged_test.save_to_disk(test_tmp_path)\n",
        "\n",
        "    if os.path.exists(test_processed_path):\n",
        "        shutil.rmtree(test_processed_path)\n",
        "    shutil.move(test_tmp_path, test_processed_path)\n",
        "\n",
        "    print(\"Updated processed test dataset saved.\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23aE-ouu8nse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce72cd16-e334-482b-d19b-a56cf05c0805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters in vocab: 137\n",
            "Total output classes (with CTC blank): 138\n",
            "CTC blank index: 137\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "processed_train_dataset = load_from_disk(processed_path)\n",
        "processed_val_dataset = load_from_disk(test_processed_path)\n",
        "\n",
        "char2id_path = '/content/drive/MyDrive/hf_ocr_data/char2id.json'\n",
        "id2char_path = '/content/drive/MyDrive/hf_ocr_data/id2char.json'\n",
        "\n",
        "with open(char2id_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    char2id = json.load(f)\n",
        "with open(id2char_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    id2char = json.load(f)\n",
        "\n",
        "id2char = {int(k): v for k, v in id2char.items()}\n",
        "\n",
        "num_classes = len(char2id) + 1\n",
        "ctc_blank_id = len(char2id)\n",
        "\n",
        "print(f\"Characters in vocab: {len(char2id)}\")\n",
        "print(f\"Total output classes (with CTC blank): {num_classes}\")\n",
        "print(f\"CTC blank index: {ctc_blank_id}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7-kwuheAldW"
      },
      "outputs": [],
      "source": [
        "class TibetanImageTensorDataset(Dataset):\n",
        "    def __init__(self, data, target_height=64, target_width=768, max_label_len=128):\n",
        "        self.target_height = target_height\n",
        "        self.target_width = target_width\n",
        "        self.data = data.filter(lambda x: len(x['label_ids']) <= max_label_len and len(x['label_ids']) > 0)\n",
        "        self.data.set_format(type='torch', columns=['image_tensor', 'label_ids'])\n",
        "        print(f\"Dataset loaded: {len(self.data)} samples (after filtering label length ≤ {max_label_len})\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image = item['image_tensor']\n",
        "        label = item['label_ids']\n",
        "        if image.ndim == 2:\n",
        "            image = image.unsqueeze(0)\n",
        "        image = F.interpolate(\n",
        "            image.unsqueeze(0),\n",
        "            size=(self.target_height, self.target_width),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        ).squeeze(0)\n",
        "        return image, label, len(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ4yW-_hAo7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b14081-efa5-4074-8873-052c2cbe2701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 12146 samples (after filtering label length ≤ 128)\n",
            "Dataset loaded: 12103 samples (after filtering label length ≤ 128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = TibetanImageTensorDataset(processed_train_dataset)\n",
        "val_ds = TibetanImageTensorDataset(processed_val_dataset)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = torch.stack([x[0] for x in batch])  # [B, C, H, W]\n",
        "    labels = [x[1] for x in batch]\n",
        "    label_lengths = torch.tensor([x[2] for x in batch], dtype=torch.long) # helps ignore the padding in loss calculation by using the lengths\n",
        "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=0) # pad the labels to the max length in the batch\n",
        "    return images, padded_labels, label_lengths\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDk-5kKaAr-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b167792b-0132-4bff-f59b-a4fbb9cedc6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 64, 768])\n",
            "torch.Size([32, 113])\n",
            "tensor([ 63,  60,  10,  72,  60,  65,  70,  69,  58,  61,  12,  74,  57,  72,\n",
            "         59,  67,  65,  63,  65,  73,  55, 113,  67,  51,  66,  76,  55,  23,\n",
            "         77,  65,  61,  31])\n"
          ]
        }
      ],
      "source": [
        "images, padded_labels, label_lengths = next(iter(train_loader))\n",
        "print(images.shape)\n",
        "print(padded_labels.shape)\n",
        "print(label_lengths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miwp2bJ5Awzj"
      },
      "outputs": [],
      "source": [
        "all_label_ids = []\n",
        "for sample in processed_train_dataset:\n",
        "    all_label_ids.extend(sample[\"label_ids\"])\n",
        "\n",
        "max_label_id = max(all_label_ids)\n",
        "print(\"Maximum label ID found in dataset:\", max_label_id)\n",
        "print(\"Vocabulary size (char2id):\", len(char2id))\n",
        "print(\"CTC blank index (should be vocab size):\", ctc_blank_id)\n",
        "assert max_label_id < ctc_blank_id, \"Your label IDs exceed expected vocab size!\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(char2id) + 1         # real chars + CTC blank\n",
        "ctc_blank_id = len(char2id)            # blank is last class\n",
        "\n",
        "class MiniResNetCTC(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MiniResNetCTC, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "        self.linear = nn.Linear(1024, 128)\n",
        "        self.classifier = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = x.reshape(x.size(0), x.size(1), -1)\n",
        "        x = self.linear(x)\n",
        "        x = self.classifier(x)\n",
        "        return x.log_softmax(dim=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MiniResNetCTC(num_classes=num_classes).to(device)"
      ],
      "metadata": {
        "id": "KvLoreHZNGXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBxvq-90Azf8"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CTCLoss(blank=ctc_blank_id, zero_infinity=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "print(processed_train_dataset[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xz6jdILOA2iD"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = TibetanImageTensorDataset(processed_train_dataset)\n",
        "val_ds   = TibetanImageTensorDataset(processed_val_dataset)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "for images, labels, label_lengths in train_loader:\n",
        "    print(\"Images:\", type(images), \"Is Tensor:\", torch.is_tensor(images))\n",
        "    print(\"Labels:\", type(labels), \"Is Tensor:\", torch.is_tensor(labels))\n",
        "    print(\"Label Lengths:\", type(label_lengths), \"Is Tensor:\", torch.is_tensor(label_lengths))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liVC9gktA5Yb"
      },
      "outputs": [],
      "source": [
        "def ctc_greedy_decode_single(pred_ids, blank):\n",
        "    decoded = []\n",
        "    prev = None\n",
        "    for i in pred_ids:\n",
        "        if i != blank and i != prev:\n",
        "            decoded.append(i)\n",
        "        prev = i\n",
        "    return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1bzmR3hIqim"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, val_loader, id2char, device):\n",
        "    model.eval()\n",
        "    exact_matches = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            images, labels, label_lengths = batch\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            log_probs = outputs.log_softmax(2)\n",
        "            decoded = log_probs.argmax(2)\n",
        "\n",
        "            batch_size = decoded.size(0)\n",
        "\n",
        "            # Decode batch predictions to strings\n",
        "            for i in range(batch_size):\n",
        "                pred_ids = decoded[i].cpu().numpy()\n",
        "                pred_decoded = ctc_greedy_decode_single(pred_ids, blank=ctc_blank_id)\n",
        "                pred_text = ''.join([id2char.get(idx, '') for idx in pred_decoded])\n",
        "                label_ids = labels[i][:label_lengths[i]].cpu().numpy()\n",
        "                true_text = ''.join([id2char.get(idx, '') for idx in label_ids])\n",
        "                if pred_text == true_text:\n",
        "                    exact_matches += 1\n",
        "                total += 1\n",
        "\n",
        "    accuracy = exact_matches / total if total > 0 else 0.0\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q93JQBVjA7TQ"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    skipped_batches = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        if batch is None:\n",
        "            skipped_batches += 1\n",
        "            continue\n",
        "\n",
        "        images, labels, label_lengths = batch\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        label_lengths = label_lengths.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        log_probs = outputs.log_softmax(2).permute(1, 0, 2)  # -> [T, B, C]\n",
        "        T = log_probs.size(0)\n",
        "        B = log_probs.size(1)\n",
        "\n",
        "        input_lengths = torch.full(size=(B,), fill_value=T, dtype=torch.long, device=device)\n",
        "\n",
        "        # Filter out invalid samples\n",
        "        if (label_lengths > T).any():\n",
        "            print(f\"Skipping batch: label length > input length\")\n",
        "            skipped_batches += 1\n",
        "            continue\n",
        "\n",
        "        if (label_lengths == 0).any():\n",
        "            print(f\"Skipping batch: empty label found\")\n",
        "            skipped_batches += 1\n",
        "            continue\n",
        "\n",
        "        valid_indices = [i for i in range(B) if label_lengths[i] > 0]\n",
        "        if not valid_indices:\n",
        "            print(\"All labels in this batch are empty, skipping batch.\")\n",
        "            skipped_batches += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Flatten valid label sequences\n",
        "            flattened_labels = torch.cat([\n",
        "                labels[i, :label_lengths[i]] for i in valid_indices\n",
        "            ])\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to flatten labels: {e}\")\n",
        "            skipped_batches += 1\n",
        "            continue\n",
        "\n",
        "        # CTC loss\n",
        "        loss = criterion(log_probs, flattened_labels, input_lengths, label_lengths)\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"Skipping batch: loss is NaN\")\n",
        "            skipped_batches += 1\n",
        "            continue\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    num_processed_batches = len(train_loader) - skipped_batches\n",
        "    if num_processed_batches == 0:\n",
        "        print(\"All batches skipped! Check your data and model.\")\n",
        "        break\n",
        "\n",
        "    average_loss = total_loss / num_processed_batches\n",
        "    loss_history.append(average_loss)\n",
        "\n",
        "    #accuracy\n",
        "    val_accuracy = evaluate(model, val_loader, id2char, device)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} | Loss: {average_loss:.4f} | \"\n",
        "          f\"Validation Accuracy: {val_accuracy*100:.2f}% | Skipped Batches: {skipped_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FI6_fw-A9VG"
      },
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def char_level_accuracy(pred, target):\n",
        "    \"\"\"\n",
        "    Computes character-level similarity between prediction and ground truth.\n",
        "    Returns a float in [0, 1].\n",
        "    \"\"\"\n",
        "    if len(target) == 0:\n",
        "        return 0.0\n",
        "    matcher = SequenceMatcher(None, pred, target)\n",
        "    return matcher.ratio()\n",
        "\n",
        "def ctc_greedy_decode_batch(pred_ids, blank):\n",
        "    \"\"\"\n",
        "    Applies greedy decoding with CTC post-processing (remove duplicates + blanks).\n",
        "    Accepts a batch of predictions and returns a list of decoded ID sequences.\n",
        "    \"\"\"\n",
        "    decoded_texts = []\n",
        "    for pred in pred_ids:\n",
        "        prev_id = blank\n",
        "        decoded = []\n",
        "        for idx in pred:\n",
        "            if idx != blank and idx != prev_id:\n",
        "                decoded.append(idx)\n",
        "            prev_id = idx\n",
        "        decoded_texts.append(decoded)\n",
        "    return decoded_texts\n",
        "\n",
        "def evaluate_character_accuracy(model, val_loader, id2char, device):\n",
        "    model.eval()\n",
        "    total_score = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            images, labels, label_lengths = batch\n",
        "            images = images.to(device)\n",
        "\n",
        "            outputs = model(images)  # [B, T, C]\n",
        "            pred_ids = outputs.argmax(2).cpu().numpy()  # [B, T]\n",
        "\n",
        "            decoded_batch = ctc_greedy_decode_batch(pred_ids, blank=ctc_blank_id)\n",
        "\n",
        "            for i in range(images.size(0)):\n",
        "                pred_text = ''.join([id2char.get(idx, '') for idx in decoded_batch[i]])\n",
        "\n",
        "                label_ids = labels[i][:label_lengths[i]].cpu().numpy()\n",
        "                true_text = ''.join([id2char.get(idx, '') for idx in label_ids])\n",
        "\n",
        "                acc = char_level_accuracy(pred_text, true_text)\n",
        "                total_score += acc\n",
        "                total_samples += 1\n",
        "\n",
        "    return total_score / total_samples if total_samples > 0 else 0.0\n",
        "\n",
        "char_acc = evaluate_character_accuracy(model, val_loader, id2char, device)\n",
        "print(f\"Validation Character-Level Accuracy: {char_acc * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((64, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# image upload\n",
        "uploaded_files = files.upload()\n",
        "img_filename = next(iter(uploaded_files.keys()))\n",
        "\n",
        "img = cv2.imread(img_filename)\n",
        "if img is None:\n",
        "    raise FileNotFoundError(f\"Image not found or failed to load: {img_filename}\")\n",
        "img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "image_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image_tensor)  # [B, T, C]\n",
        "    output = output.log_softmax(2)\n",
        "    pred_ids = output.argmax(2).squeeze(0).cpu().numpy()  # [T]\n",
        "\n",
        "decoded_ids = ctc_greedy_decode_single(pred_ids, blank=ctc_blank_id)\n",
        "\n",
        "#blank info\n",
        "print(f\"CTC blank index: {ctc_blank_id}\")\n",
        "if ctc_blank_id in id2char:\n",
        "    print(f\"Warning: Blank index maps to: {repr(id2char[ctc_blank_id])}\")\n",
        "else:\n",
        "    print(\"Blank index is not in id2char (safe)\")\n",
        "\n",
        "# character frequency\n",
        "from collections import Counter\n",
        "counter = Counter(decoded_ids)\n",
        "print(\"Top 10 predicted character IDs and frequencies:\")\n",
        "for idx, count in counter.most_common(10):\n",
        "    ch = id2char.get(idx, '?')\n",
        "    print(f\"ID {idx}: {repr(ch)} — {count} times\")\n",
        "\n",
        "pred_text = ''.join([id2char[i] for i in decoded_ids if i in id2char])\n",
        "print(\"Final Predicted Text:\", pred_text)\n"
      ],
      "metadata": {
        "id": "vBnnDIFhF80L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMeyu3HU9qHyswVLiaa4dw5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}